<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[selenium 使用代理的方法汇总]]></title>
    <url>%2F%2Fp%2F9202ce96.html</url>
    <content type="text"><![CDATA[在docker中启动selenium gird使用扩展，并使用隧道代理，比如阿布云、多贝云、蘑菇代理。how to set proxy with authentication in selenium chromedriver python proxy with authentication(账号密码认证代理)不支持chrome headless，但是对docker selenium 或者 selenium gird集群，是支持的。启动selenium docker 1docker run -d -p 4444:4444 --shm-size=2g -m 800M --memory-swap=800M --name=chrome --restart=always selenium/standalone-chrome 一、selenium使用隧道动态代理(会生成本地zip插件文件)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114import osimport timeimport zipfilefrom selenium import webdriverfrom scrapy.selector import SelectorPROXY_HOST = 'http-dyn.abuyun.com' # rotating proxy or hostPROXY_PORT = 9020 # portPROXY_USER = '' # usernamePROXY_PASS = '' # passwordREMOTE_SELENIUM = '111.22.111.11:4444' # 远端docker selenium地址manifest_json = """&#123; "version": "1.0.0", "manifest_version": 2, "name": "Chrome Proxy", "permissions": [ "proxy", "tabs", "unlimitedStorage", "storage", "&lt;all_urls&gt;", "webRequest", "webRequestBlocking" ], "background": &#123; "scripts": ["background.js"] &#125;, "minimum_chrome_version":"22.0.0"&#125;"""background_js = """var config = &#123; mode: "fixed_servers", rules: &#123; singleProxy: &#123; scheme: "http", host: "%s", port: parseInt(%s) &#125;, bypassList: ["localhost"] &#125; &#125;;chrome.proxy.settings.set(&#123;value: config, scope: "regular"&#125;, function() &#123;&#125;);function callbackFn(details) &#123; return &#123; authCredentials: &#123; username: "%s", password: "%s" &#125; &#125;;&#125;chrome.webRequest.onAuthRequired.addListener( callbackFn, &#123;urls: ["&lt;all_urls&gt;"]&#125;, ['blocking']);""" % (PROXY_HOST, PROXY_PORT, PROXY_USER, PROXY_PASS)def get_chromedriver(use_proxy=False, user_agent=None, use_docker=True): path = os.path.dirname(os.path.abspath(__file__)) chrome_options = webdriver.ChromeOptions() if use_proxy: pluginfile = 'proxy_auth_plugin.zip' with zipfile.ZipFile(pluginfile, 'w') as zp: zp.writestr("manifest.json", manifest_json) zp.writestr("background.js", background_js) chrome_options.add_extension(pluginfile) if user_agent: chrome_options.add_argument('--user-agent=%s' % user_agent) if use_docker: driver = webdriver.Remote( command_executor="http://&#123;&#125;/wd/hub".format(REMOTE_SELENIUM), # command_executor="http://192.168.95.56:4444/wd/hub", options=chrome_options ) else: driver = webdriver.Chrome( os.path.join(path, '/usr/local/bin/chromedriver'), chrome_options=chrome_options) return driverdef main(): # 使用代理 使用docker driver = get_chromedriver(use_proxy=True, use_docker=True) print(driver) n = 0 while True: # driver = get_chromedriver(use_proxy=True, use_docker=True) # print(driver) driver.get('https://www.cip.cc') ip_text = Selector(text=driver.page_source).xpath( '//pre/text()').extract_first().strip() print(ip_text) driver.close() time.sleep(3) n += 1 if n &gt; 10: break driver.quit()if __name__ == '__main__': main() 效果图 二、selenium 使用芝麻代理等常规HOST:PORT代理123456789from selenium import webdriverPROXY = "88.157.149.250:8080" # IP:PORT or HOST:PORTchrome_options = webdriver.ChromeOptions()chrome_options.add_argument('--proxy-server=%s' % PROXY)chrome = webdriver.Chrome(chrome_options=chrome_options)chrome.get("http://www.cip.cc")print(chrome.page_source)]]></content>
      <categories>
        <category>「原创分享」</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[项目管理碎碎念：自动生成周报]]></title>
    <url>%2F%2Fp%2F603a0fc5.html</url>
    <content type="text"><![CDATA[项目管理碎碎念：自动生成周报需求管理2-3个人的团队，需求如下： 需要明确团队的任务 将团队的任务wbs，分解成每个人的任务 每周对上级进行周报汇报 想得到的目标： 明确的看板任务，提升团队效率 写总结报告的时候，不用纠结回忆，节省时间和心力，也不会遗漏 主角Trello + Planyway Trello的任务看板适合Get things done，随时添加零碎项目，提升效率。 Planyway的甘特图适合对整个工作阶段有个全局的认识 Planyway 官网：https://planyway.com/ Chrome extension 下载地址：https://chrome.google.com/webstore/detail/planyway-team-planner-for/kkgaechmpjgbojahkofamdjkaklgbdkc 效果图Planyway生成一周的甘特图看板，进行周报汇总或者参考，当然也可以直接打印 Trello钉钉提醒： 特点 工具免费：个人使用者的最大需求，秒杀 轻量、好学：比公司自建的jira、禅道 轻量，比omniplan好学 提醒及时：Trello生态强大，结合钉钉、slack等工具，提醒及时 结语安利大家可以用用，把工作搞得整整齐齐]]></content>
      <categories>
        <category>「原创分享」</category>
      </categories>
      <tags>
        <tag>项目管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[把布隆过滤器用起来]]></title>
    <url>%2F%2Fp%2F12b51cde.html</url>
    <content type="text"><![CDATA[把布隆过滤器用起来本文偏应用和代码实践，理论请参考本文末尾参考文章 简介一句话简介：过滤器，判断这个元素在与不在，不在则100%不在；在则去查询，确认在不在。 详细简介：BloomFilter，中文名称叫做布隆过滤器，是1970年由 Bloom 提出的，它可以被用来检测一个元素是否在一个集合中，它的空间利用效率很高，使用它可以大大节省存储空间。BloomFilter 使用位数组表示一个待检测集合，并可以快速地通过概率算法判断一个元素是否存在于这个集合中，所以利用这个算法我们可以实现去重效果。 它的优点是空间效率和查询时间都远远超过一般算法，缺点是有一定的误识别率和删除困难。 场景1、大量爬虫数据去重 2、保护数据安全：广告精确投放 ：广告主通过设备id，计算hash算法，在数据包（数据提供方）中去查找，如果在存在，则证明该设备id属于目标人群，进行投放广告，同时保证设备id不泄露。数据提供方和广告主都没有暴露自己拥有的设备id。间接用户画像且不违数据安全法。详见：https://zhuanlan.zhihu.com/p/37847480 3、比特币网络转账确认 SPV节点:SPV是“Simplified Payment Verification”（简单支付验证）的缩写。中本聪论文简要地提及了这一概念，指出：不运行完全节点也可验证支付，用户只需要保存所有的block header就可以了。用户虽然不能自己验证交易，但如果能够从区块链的某处找到相符的交易，他就可以知道网络已经认可了这笔交易，而且得到了网络的多少个确认。 先去访问布隆过滤器，去判断交易记录是否在某个block(区块)里存在。从海量数据(十亿个区块，每个区块1-2M的交易记录，)，快速得到结果。详见:https://www.youtube.com/watch?v=uC6Q5m0SSQ0 4、分布式系统（Map-Reduce）把大任务切分成块，分配和验证一个子任务是否在一个子系统上。 必要性省空间，提升效率 我们首先来回顾一下 ScrapyRedis 的去重机制，它将 Request 的指纹存储到了 Redis 集合中，每个指纹的长度为 40，例如 27adcc2e8979cdee0c9cecbbe8bf8ff51edefb61 就是一个指纹，它的每一位都是 16 进制数。 让我们来计算一下用这种方式耗费的存储空间，每个 16 进制数占用 4b，1 个指纹用 40 个 16 进制数表示，占用空间为 20B，所以 1 万个指纹即占用空间 200KB，1 亿个指纹即占用 2G，所以当我们的爬取数量达到上亿级别时，Redis 的占用的内存就会变得很高，而且这仅仅是指纹的存储，另外 Redis 还存储了爬取队列，内存占用会进一步提高，更别说有多个 Scrapy 项目同时爬取的情况了。所以当爬取达到亿级别规模时 ScrapyRedis 提供的集合去重已经不能满足我们的要求，所以在这里我们需要使用一个更加节省内存的去重算法，它叫做 BloomFilter。 (内存版)Python实现的内存版布隆过滤器pybloomhttps://github.com/jaybaird/python-bloomfilter安装:1pip install pybloom 该模块包含两个类实现布隆过滤器功能。BloomFilter 是定容。ScalableBloomFilter 可以自动扩容 使用:1234567891011121314151617&gt;&gt;&gt; from pybloom import BloomFilter&gt;&gt;&gt; f = BloomFilter(capacity=1000, error_rate=0.001) # capacity是容量, error_rate 是能容忍的误报率&gt;&gt;&gt; f.add('Traim304') # 当不存在该元素,返回FalseFalse&gt;&gt;&gt; f.add('Traim304') # 若存在,返回 TrueTrue&gt;&gt;&gt; 'Traim304' in f # 值得注意的是若返回 True。该元素可能存在, 也可能不存在。过滤器能容许存在一定的错误True&gt;&gt;&gt; 'Jacob' in f # 但是 False。则必定不存在False&gt;&gt;&gt; len(f) # 当前存在的元素1&gt;&gt;&gt; f = BloomFilter(capacity=1000, error_rate=0.001) &gt;&gt;&gt; from pybloom import ScalableBloomFilter&gt;&gt;&gt; sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_GROWTH)&gt;&gt;&gt; # sbf.add() 与 BloomFilter 同 超过误报率时抛出异常123456789101112131415161718&gt;&gt;&gt; f = BloomFilter(capacity=1000, error_rate=0.0000001)&gt;&gt;&gt; for a in range(1000):... _ = f.add(a)...&gt;&gt;&gt; len(a)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: object of type 'int' has no len()&gt;&gt;&gt; len(f)1000&gt;&gt;&gt; f.add(1000)False&gt;&gt;&gt; f.add(1001) # 当误报率超过 error_rate 会报错Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/usr/local/lib/python2.7/site-packages/pybloom/pybloom.py", line 182, in add raise IndexError("BloomFilter is at capacity")IndexError: BloomFilter is at capacity (持久化)手动实现的redis版布隆过滤器大数据量，多用Redis持久化版本的布隆过滤器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# 布隆过滤器 redis版本实现import hashlibimport redisimport six# 1. 多个hash函数的实现和求值# 2. hash表实现和实现对应的映射和判断class MultipleHash(object): '''根据提供的原始数据，和预定义的多个salt，生成多个hash函数值''' def __init__(self, salts, hash_func_name="md5"): self.hash_func = getattr(hashlib, hash_func_name) if len(salts) &lt; 3: raise Exception("请至少提供3个salt") self.salts = salts def get_hash_values(self, data): '''根据提供的原始数据, 返回多个hash函数值''' hash_values = [] for i in self.salts: hash_obj = self.hash_func() hash_obj.update(self._safe_data(data)) hash_obj.update(self._safe_data(i)) ret = hash_obj.hexdigest() hash_values.append(int(ret, 16)) return hash_values def _safe_data(self, data): ''' python2 str === python3 bytes python2 uniocde === python3 str :param data: 给定的原始数据 :return: 二进制类型的字符串数据 ''' if six.PY3: if isinstance(data, bytes): return data elif isinstance(data, str): return data.encode() else: raise Exception("请提供一个字符串") # 建议使用英文来描述 else: if isinstance(data, str): return data elif isinstance(data, unicode): return data.encode() else: raise Exception("请提供一个字符串") # 建议使用英文来描述class BloomFilter(object): '''''' def __init__(self, salts, redis_host="localhost", redis_port=6379, redis_db=0, redis_key="bloomfilter"): self.redis_host = redis_host self.redis_port = redis_port self.redis_db = redis_db self.redis_key = redis_key self.client = self._get_redis_client() self.multiple_hash = MultipleHash(salts) def _get_redis_client(self): '''返回一个redis连接对象''' pool = redis.ConnectionPool(host=self.redis_host, port=self.redis_port, db=self.redis_db) client = redis.StrictRedis(connection_pool=pool) return client def save(self, data): '''''' hash_values = self.multiple_hash.get_hash_values(data) for hash_value in hash_values: offset = self._get_offset(hash_value) self.client.setbit(self.redis_key, offset, 1) return True def is_exists(self, data): hash_values = self.multiple_hash.get_hash_values(data) for hash_value in hash_values: offset = self._get_offset(hash_value) v = self.client.getbit(self.redis_key, offset) if v == 0: return False return True def _get_offset(self, hash_value): # 512M长度哈希表 # 2**8 = 256 # 2**20 = 1024 * 1024 # (2**8 * 2**20 * 2*3) 代表hash表的长度 如果同一项目中不能更改 return hash_value % (2**8 * 2**20 * 2*3)if __name__ == '__main__': data = ["asdfasdf", "123", "123", "456","asf", "asf"] bm = BloomFilter(salts=["1","2","3", "4"],redis_host="172.17.0.2") for d in data: if not bm.is_exists(d): bm.save(d) print("映射数据成功： ", d) else: print("发现重复数据：", d) 应用在scrapy-redis中代码已经打包成了一个 Python 包并发布到了 PyPi，链接为：https://pypi.python.org/pypi/scrapy-redis-bloomfilter，因此我们以后如果想使用 ScrapyRedisBloomFilter 直接使用就好了，不需要再自己实现一遍。 我们可以直接使用Pip来安装，命令如下：1pip3 install scrapy-redis-bloomfilter 使用的方法和 ScrapyRedis 基本相似，在这里说明几个关键配置：123456# 去重类，要使用BloomFilter请替换DUPEFILTER_CLASSDUPEFILTER_CLASS = "scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter"# 哈希函数的个数，默认为6，可以自行修改BLOOMFILTER_HASH_NUMBER = 6# BloomFilter的bit参数，默认30，占用128MB空间，去重量级1亿BLOOMFILTER_BIT = 30 DUPEFILTER_CLASS 是去重类，如果要使用 BloomFilter需要将 DUPEFILTER_CLASS 修改为该包的去重类。 BLOOMFILTER_HASH_NUMBER 是 BloomFilter 使用的哈希函数的个数，默认为 6，可以根据去重量级自行修改。 BLOOMFILTER_BIT 即前文所介绍的 BloomFilter 类的 bit 参数，它决定了位数组的位数，如果 BLOOMFILTER_BIT 为 30，那么位数组位数为 2 的 30次方，将占用 Redis 128MB 的存储空间，去重量级在 1 亿左右，即对应爬取量级 1 亿左右。如果爬取量级在 10亿、20 亿甚至 100 亿，请务必将此参数对应调高。 测试12345678910111213141516171819Spider 文件:from scrapy import Request, Spiderclass TestSpider(Spider): name = 'test' base_url = 'https://www.baidu.com/s?wd=' def start_requests(self): for i in range(10): url = self.base_url + str(i) yield Request(url, callback=self.parse) # Here contains 10 duplicated Requests for i in range(100): url = self.base_url + str(i) yield Request(url, callback=self.parse) def parse(self, response): self.logger.debug('Response of ' + response.url) 在 start_requests() 方法中首先循环 10 次，构造参数为 0-9 的 URL，然后重新循环了 100 次，构造了参数为 0-99 的 URL，那么这里就会包含 10 个重复的 Request，我们运行项目测试一下：1scrapy crawl test 可以看到最后的输出结果如下：1234567891011121314151617&#123;'bloomfilter/filtered': 10, 'downloader/request_bytes': 34021, 'downloader/request_count': 100, 'downloader/request_method_count/GET': 100, 'downloader/response_bytes': 72943, 'downloader/response_count': 100, 'downloader/response_status_count/200': 100, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2017, 8, 11, 9, 34, 30, 419597), 'log_count/DEBUG': 202, 'log_count/INFO': 7, 'memusage/max': 54153216, 'memusage/startup': 54153216, 'response_received_count': 100, 'scheduler/dequeued/redis': 100, 'scheduler/enqueued/redis': 100, 'start_time': datetime.datetime(2017, 8, 11, 9, 34, 26, 495018)&#125; 可以看到最后统计的第一行的结果：1'bloomfilter/filtered': 10, 这就是 BloomFilter 过滤后的统计结果，可以看到它的过滤个数为 10 个，也就是它成功将重复的 10 个 Reqeust 识别出来了，测试通过。 原理本文偏应用，难以描述的原理，最后说。一个很长的二进制向量和一个映射函数。 参考资料1、https://zhuanlan.zhihu.com/p/378474802、https://www.youtube.com/watch?v=uC6Q5m0SSQ03、《python3网络爬虫开发实战》崔庆才4、https://www.jianshu.com/p/f57187e2b5b9]]></content>
      <categories>
        <category>「原创分享」</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[02Python中一切皆对象——Python高级编程和异步IO并发编程]]></title>
    <url>%2F%2Fp%2Fc6651072.html</url>
    <content type="text"><![CDATA[02Python中一切皆对象——Python高级编程和异步IO并发编程2.1 Python中一切皆是对象讲解动态语言和静态语言的区别Python的面向对象更彻底函数和类也是对象，属于Python的一等公民1、赋值给一个变量2、可以添加到集合对象中3、可以作为参数传递给函数4、可以当做函数的返回值2.2 type、object和class的关系2.3 Python中的常见内置类型2.4 本章小结]]></content>
      <categories>
        <category>「笔记」</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[01课程介绍——Python高级编程和异步IO并发编程]]></title>
    <url>%2F%2Fp%2F2ed8a5ab.html</url>
    <content type="text"><![CDATA[01课程介绍——Python高级编程和异步IO并发编程为什么需要进阶Python高级工程师会面试什么我们只需要知道当前的Python知识就够了吗？语言本身的进阶知识优先于框架，大公司更注重语言本身的功底 如何进阶和大神一起阅读优秀源码、懂原理面试、多做项目 你是否遇到看不懂优秀库和框架的源码不知道如何才能进一步优化自己的代码asyncio、tornado 等异步框架背后的原理Python代码灵活背后的设计原理对生成器稀里糊涂很多Python抛出的异常看不懂 课程概述目标：系统全面学习Python高级知识和并发编程方法：功能-&gt;原理-&gt;应用案例技术：面向对象、魔法方法、元类、生成器、多线程-&gt;协程 章节安排01 课程介绍02 Python中一切皆对象03 魔法方法04 深入类和对象05 自定义序列类06 深入Python的set和dict07 对象引用、可变性和垃圾回收08 元类编程09 迭代器和生成器10 Python socket编程11 多线程、多进程和线程池编程12、协程和异步IO13、asyncio并发编程14、课程总结课程安排一切皆对象魔法方法详解Python序列协议深入dict和set迭代器和生成器详解socket编程详解对象引用和可变性、垃圾回收元类编程多线程（池）、多进程（池）异步io和协程asyncio并发编程课程安排-面向对象鸭子类型抽象基类MRO属性查找算法和super函数静态方法、类方法、实例方法数据封装和私有属性对象的自省机制上下文管理器contextlib实现上下文管理器mixin继承模式的应用课程安排-元类property动态属性getattr、getattribute的区别属性描述符new和init元类实现orm课程安排-多线程GIL和多线程线程通信-共享变量、Queue线程同步-Lock、RLock、Condition、Semaphores线程池和源码分析-ThreadPoolExecutor多进程-multiprocessing进程间通信课程安排-异步IOIO多路复用-select、poll、epollselect+回调+事件循环模式生成器进阶-send、close、throw和yield fromasync和await课程安排-asyncioFuture和Taskaiohttp实现高并发抓取urlasyncio 背后的selector协程同步和通信ThreadPoolExecutor + asyncio]]></content>
      <categories>
        <category>「笔记」</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[scrapy-spalsh使用UA和IP代理]]></title>
    <url>%2F%2Fp%2Fc8c843a9.html</url>
    <content type="text"><![CDATA[scrapy-spalsh使用UA和IP代理核心设置UA，优先在lua脚本中使用splash:set_user_agent(“{ua}”) 设置ip代理，使用SplashRequest的proxy 代码1pip install fake-useragent 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# -*- coding: utf-8 -*-import scrapyfrom scrapy import Spiderfrom urllib.parse import quotefrom scrapy_splash import SplashRequestfrom risk_control_info.utils import get_proxy_ipfrom fake_useragent import UserAgentua = UserAgent()script = """function main(splash, args) splash.images_enabled = false splash:set_user_agent("&#123;ua&#125;") assert(splash:go(args.url)) assert(splash:wait(args.wait)) return splash:html()end""".format(ua=ua.chrome)class AppQimaiHotSearchSpider(scrapy.Spider): name = 'app_qimai_hot_search' allowed_domains = ['qimai.cn'] user_agent = ua.chrome custom_settings = &#123; 'DOWNLOADER_MIDDLEWARES': &#123; 'scrapy_splash.SplashCookiesMiddleware': 723, 'scrapy_splash.SplashMiddleware': 725, 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810, &#125;, 'SPIDER_MIDDLEWARES': &#123; 'scrapy_splash.SplashDeduplicateArgsMiddleware': 100, &#125;, &#125; def start_requests(self): url = "http://httpbin.org/get" yield SplashRequest(url=url, callback=self.parse, endpoint='execute', args=&#123; 'lua_source': script, 'proxy': "http://" + get_proxy_ip(url), 'wait': 3&#125;) def parse(self, response): print(response.body.decode()) 结果]]></content>
      <categories>
        <category>「原创分享」</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[00 机器学习概述、特征工程、机器学习算法]]></title>
    <url>%2F%2Fp%2F14207b93.html</url>
    <content type="text"><![CDATA[00 机器学习概述、特征工程、机器学习算法引入：”人工智能之父“ 艾伦图灵：图灵测试 马文·李·闵斯基（英语：Marvin Lee Minsky，1927年8月9日－2016年1月24日），科学家专长于认知科学与人工智能领域，麻省理工学院人工智能实验室的创始人之一，著有几部人工智能和哲学方面的作品。1969年，因为在人工智能领域的贡献，获得图灵奖。 关系 新闻一“小”一“同”南方都市报的“小南”，广州日报的“阿同”机器人 技术方面的话，主要是计算机视觉，自然语言处理，数据挖掘。 计算机视觉就包括图像识别，视频识别，具体应用有人脸识别，步态识别，无人驾驶汽车等等。 自然语言处理包括机器翻译，语音识别，文本挖掘等等，像siri，谷歌翻译里面都有很多的自然语言处理技术。 数据挖掘主要是各种推荐和预测，包括电子商务的商品推荐，计算广告，社交网络分析（微博好友推荐等），预测一些趋势，比如股市的走向，天气的变化等。 无人驾驶百度：阿波罗 医疗 医疗CT 量化交易图片艺术化 GAN神经网络：视觉相关处理 Ai智能手机 智能推荐 开发框架pytorch TensorFlow： scikit-learn： 推荐书籍和知识背景 机器学习概述什么是 机器学习机器学习是从数据中自动分析获得规律（模型），并利用规律对未知数据进行预测 案例：分辨啤酒还是红酒 搜集数据： 准备数据 为什么 需要机器学习怎么用 机器学习（应用场景）数据来源与类型数据的特征工程机器学习基础]]></content>
      <categories>
        <category>「笔记」</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Centos7使用supervisor]]></title>
    <url>%2F%2Fp%2F8961b837.html</url>
    <content type="text"><![CDATA[在Centos7使用supervisor置顶：推荐使用pip install supervisor安装最新版supervisor，yum install supervisor最高版本3.1.4，有很多bug 名称解释supervisor：要安装的软件的名称。supervisord：装好supervisor软件后，supervisord用于启动supervisor服务。supervisorctl：用于管理supervisor配置文件中program。 Centos7 Python3.7安装最新版supervisorCentos7 通过编译安装的Python3.7 pip37 安装supervisor最新版。pyhton3支持日志中文，python3配合最新版supervisor，bug最少，体验最佳。 12345678➜ ~ pip37 install supervisor Looking in indexes: https://mirrors.aliyun.com/pypi/simpleCollecting supervisor Downloading https://mirrors.aliyun.com/pypi/packages/ca/1f/07713b0e1e34c312450878801d496bce8b9eff5ea9e70d41ff4e299b2df5/supervisor-4.1.0-py2.py3-none-any.whl (318kB) |████████████████████████████████| 327kB 46.5MB/s Installing collected packages: supervisorSuccessfully installed supervisor-4.1.0 卸载的时候，发现了Python3的supervisor路径 1234567891011➜ ~ pip37 uninstall supervisor Uninstalling supervisor-4.1.0: Would remove: /usr/local/python3/bin/echo_supervisord_conf /usr/local/python3/bin/pidproxy /usr/local/python3/bin/supervisorctl /usr/local/python3/bin/supervisord /usr/local/python3/lib/python3.7/site-packages/supervisor-4.1.0.dist-info/* /usr/local/python3/lib/python3.7/site-packages/supervisor/*Proceed (y/n)? y Successfully uninstalled supervisor-4.1.0 supervisor配置 123456789101112➜ ~ /usr/local/python3/bin/echo_supervisord_conf &gt; /etc/supervisor/supervisord.conf ➜ ~ vim /etc/supervisor/supervisord.conf web界面配置[inet_http_server] ; inet (TCP) server disabled by default port=*:9001 ; ip_address:port specifier, *:port for all iface username=clark ; default is no username (open server) password=123456 ; default is no password (open server) 指定配置文件目录[include] files = /etc/supervisor/config.d/*.ini 启动服务 12345➜ ~ /usr/local/python3/bin/supervisord -c /etc/supervisor/supervisord.conf ➜ ~ ps -ef|grep supervisord # 查看是否存在supervisord进程 root 16641 1 0 19:46 ? 00:00:00 /usr/local/python3/bin/python3.7 /usr/local/python3/bin/supervisord -c /etc/supervisor/supervisord.conf root 16683 16581 0 19:46 pts/1 00:00:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn supervisord 配置修改后的重新载入，代替supervisorctl reload1234567891011121314(scrapy_hub) ➜ gamble_monitor git:(master) /usr/local/python3/bin/supervisorctl -c /etc/supervisor/supervisord.conf update_api_run RUNNING pid 22136, uptime 0:09:04supervisor&gt; reload Really restart the remote supervisord process y/N? yRestarted supervisordsupervisor&gt; help default commands (type help &lt;topic&gt;):=====================================add exit open reload restart start tail avail fg pid remove shutdown status update clear maintail quit reread signal stop versionsupervisor&gt; 分割线：下面供参考，yum方式安装是旧版本。使用yum安装12345678910111213141516yum install epel-releaseyum install supervisorsystemctl enable supervisord.service # 开机自启动systemctl start supervisord.service # 启动supervisord服务systemctl status supervisord.service # 查看supervisord服务状态# 开启web服务:修改supervisord.conf ,inet_http_server节点取消注释。vim /etc/supervisord.conf supervisorctl reload[inet_http_server] ; inet (TCP) server disabled by defaultport=*:9001 ; (ip_address:port specifier, *:port for all iface)username=clark ; (default is no username (open server))password=123456 ; (default is no password (open server))ps -ef|grep supervisord # 查看是否存在supervisord进程 应用配置Supervisor 管理应用的进程，需要对每个应用进行配置。在 /etc/supervisord.d 中创建 helloworld.ini，每个应用对应一个配置文件即可。下面是配置文件的示例：1234567891011[program:helloworld] ;程序的名称command = dotnet HelloWorld.dll ;执行的命令directory = /root/www/ ;命令执行的目录environment = ASPNETCORE__ENVIRONMENT=Production ;环境变量user = root ;执行进程的用户stopsignal = INT autostart = true ;是否自动启动autorestart = true ;是否自动重启startsecs = 1 ;自动重启间隔stderr_logfile = /var/log/helloworld.err.log ;标准错误日志stdout_logfile = /var/log/helloworld.out.log ;标准输出日志 实际项目:12345678910111213141516supervisor部署：cd /etc/supervisord.d/vim product_new_merchant.ini[program:product_new_merchant]user = rootdirectory = /root/data_hub/all_scriptcommand = /root/.virtualenvs/ProductNewMerchant/bin/python3.7 product_new_merchant.pyautostart = trueautorestart = truestartsecs = 1environment = MYSQL_HOST=&quot;192.168.6.160&quot;,MYSQL_DATABASE=&quot;xxxxx&quot;,MYSQL_USER_NAME=&quot;xxxxxx&quot;,MYSQL_PASSWORD=&quot;xxxxx&quot;,MYSQL_PORT=3306,REDIS_HOST=&quot;192.168.95.55&quot;,LANG=&quot;en_US.utf8&quot;,LC_ALL=&quot;en_US.UTF-8&quot;,LC_LANG=&quot;en_US.UTF-8&quot;stdout_logfile = /var/log/product_new_merchant.logredirect_stderr=truestopsignal = INT 创建好配置文件后，重启 Supervisor1supervisorctl reload 或热重启，不会重启其他子进程123supervisorctl rereadsupervisorctl update 为确保没有错误，可以正常启动，使用前文提到的查看Supervisor状态的命令查看。或者查看要管理的进程是否启动，本例中可以使用下面的命令：1234ps -ef | grep HelloWorld.dll或ps -ef | grep dotnet 可能遇到的问题1、我在网页上tail的时候，要不就是一直不返回，要不就是Error responseError code 410.Message: Gone. 解决方法:可能是Chrome某个插件不兼容，影响到了supervisor。使用无痕模式或者换一个Safari浏览器，解决了。 2、stderr_logfile和stdout_logfile 颠倒3、web界面中文乱码参考：https://www.crifan.com/upgrade_centos_supervisor_to_latest_version/1234567891011121314151617181920213.1.4老版本，升级最新版[root@web-95-55 admin]# supervisord --version3.1.4备份yum版本supervisor配置文件，供参考:yum remove supervisoryum remove python-meld3pip install supervisormkdir /etc/supervisorecho_supervisord_conf &gt; /etc/supervisor/supervisord.confvim /etc/supervisor/supervisord.conf修改应用配置文件路径：;[include]files = /etc/supervisor/config.d/*.inimkdir /etc/supervisor/config.dcd /etc/supervisor/config.d启动supervisor：supervisord -c /etc/supervisor/supervisord.conf 附：公司运维小哥笔记123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960yum install supervisor -ysupervisor 配置说明通过这种形式安装的supervisor，其配置文件的目录位于：/etc/supervisord.conf (主配置文件，下面会详细介绍)/etc/supervisor.d/ (默认子进程配置文件，也就是需要我们根据程序配置的地方)[unix_http_server]file=/home/supervisor/supervisor.sock ; supervisorctl使用的 socket文件的路径;chmod=0700 ; 默认的socket文件权限0700;chown=nobody:nogroup ; socket文件的拥有者[inet_http_server] ; 提供web管理后台管理相关配置port=0.0.0.0:9001 ; web管理后台运行的ip地址及端口，绑定外网需考虑安全性 ;username=root ; web管理后台登录用户名密码;password=root[supervisord]logfile=/var/log/supervisord.log ; 日志文件，默认在$CWD/supervisord.loglogfile_maxbytes=50MB ; 日志限制大小，超过会生成新文件，0表示不限制logfile_backups=10 ; 日志备份数量默认10，0表示不备份loglevel=info ; 日志级别pidfile=/home/supervisor/supervisord.pid ; supervisord pidfile; default supervisord.pid ; pid文件nodaemon=false ; 是否在前台启动，默认后台启动falseminfds=1024 ; 可以打开文件描述符最小值minprocs=200 ; 可以打开的进程最小值[supervisorctl]serverurl=unix:///home/supervisor/supervisor.sock ; 通过socket连接supervisord,路径与unix_http_server-&gt;file配置的一致[include]files = supervisor.d/*.conf ;指定了在当前目录supervisor.d文件夹下配置多个配置文件定义supervisor管理进程配置文件[program:sboot] ;[program:xxx] 这里的xxx是指的项目名字directory = /opt/project ;程序所在目录command = java -jar springboot-hello-sample.jar ;程序启动命令autostart=true ;是否跟随supervisord的启动而启动autorestart=true; 程序退出后自动重启,可选值：[unexpected,true,false]，默认为unexpected，表示进程意外杀死后才重启stopasgroup=true;进程被杀死时，是否向这个进程组发送stop信号，包括子进程killasgroup=true;向进程组发送kill信号，包括子进程stdout_logfile=/var/log/sboot/supervisor.log;该程序日志输出文件，目录需要手动创建stdout_logfile_maxbytes = 50MB;日志大小stdout_logfile_backups = 100;备份数service supervisord restart直接在命令行输入supervisorctl会展示当前已配置好的项目信息。[root@wangzh supervisor.d]# supervisorctl sboot RUNNING pid 27517, uptime 0:18:04supervisor&gt; 然后可以执行start/stop/restart sboot 来简单控制项目的启停等supervisorctl update #更新配置文件supervisorctl reload #重新启动配置的程序supervisorctl stop all #停止全部管理进程 参考:1、https://www.chengxulvtu.com/supervisor-on-centos-7/ 2、https://blog.csdn.net/DongGeGe214/article/details/80264811]]></content>
      <categories>
        <category>「原创分享」</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Centos7安装python3、配置zsh、虚拟环境]]></title>
    <url>%2F%2Fp%2F408fe7ca.html</url>
    <content type="text"><![CDATA[安装python3参考：https://www.yuzhi100.com/tutorial/centos/centos-anzhuang-python36CentOS7系统自带的Python版本是Python2.7，如需使用Python3.6，需要自行安装Python3.6。 (1)安装IUS软件源1234#安装EPEL依赖sudo yum install epel-release#安装IUS软件源sudo yum install https://centos7.iuscommunity.org/ius-release.rpm (2)安装Python3.61sudo yum install python36u 安装Python3完成后的shell命令为python3.6，为了使用方便，创建一个到python3的符号链接123# rm /bin/python # 删除默认的python2# sudo ln -s /bin/python3.6 /bin/pythonsudo ln -s /bin/python3.6 /bin/python3 (3)安装pip3安装完成python36u并没有安装pip，安/装pip1sudo yum install python36u-pip 安装pip完成后的shell命令为pip3.6，为了使用方便，创建一个到pip3的符号链接12sudo ln -s /bin/pip3.6 /bin/pipsudo ln -s /bin/pip3.6 /bin/pip3 (4) 遇到的问题1、使用yum命令报错File “/usr/bin/yum”, line 30 except KeyboardInterrupt, e:12345678[root@10-41-140-1 ~]# yum install zsh File &quot;/usr/bin/yum&quot;, line 30 except KeyboardInterrupt, e: ^SyntaxError: invalid syntax[root@10-41-140-1 ~]# rm /bin/pythonrm: remove symbolic link ‘/bin/python’? y[root@10-41-140-1 ~]# sudo ln -s /bin/python2.7 /bin/python 问题出现原因：yum包管理是使用python2.x写的，将python2.x升级到python3.1.3以后，由于python版本语法兼容性导致问题出现解决办法： python软连接到指向python2.7参考：https://blog.csdn.net/zsl10/article/details/52315319 安装zsh参考:https://www.jianshu.com/p/556ff130fc65 (1)安装zsh包1yum -y install zsh (2)切换默认shell为zsh1chsh -s /bin/zsh (3)重启终端，让修改的配置生效(4)安装oh-my-zsh4.1 安装git1sudo yum install git 4.2 通过curl安装1curl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | sh 4.2 或者通过wget安装1wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - | sh 4.3 修改oh-my-zsh主题查看oh my zsh主题1ls ~/.oh-my-zsh/themes 修改主题1vim ~/.zshrc 可以看到默认的主题是ZSH_THEME=”robbyrussell” 改成自己喜欢的即可. 4.4 重启终端，主题生效 虚拟环境(1) 安装virtualenv相关依赖12sudo pip install virtualenvsudo pip install virtualenvwrapper (2)修改 .zshrc123➜ ~ where is virtualenvwrapper.sh is not found /usr/bin/virtualenvwrapper.sh 1234复制内容,进入.zshrc export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3export WORKON_HOME=$HOME/.virtualenvssource /usr/bin/virtualenvwrapper.sh 123456789101112131415➜ ~ vim .zshrc ➜ ~ source .zshrc virtualenvwrapper.user_scripts creating /root/.virtualenvs/premkproject virtualenvwrapper.user_scripts creating /root/.virtualenvs/postmkproject virtualenvwrapper.user_scripts creating /root/.virtualenvs/initialize virtualenvwrapper.user_scripts creating /root/.virtualenvs/premkvirtualenv virtualenvwrapper.user_scripts creating /root/.virtualenvs/postmkvirtualenv virtualenvwrapper.user_scripts creating /root/.virtualenvs/prermvirtualenv virtualenvwrapper.user_scripts creating /root/.virtualenvs/postrmvirtualenv virtualenvwrapper.user_scripts creating /root/.virtualenvs/predeactivate virtualenvwrapper.user_scripts creating /root/.virtualenvs/postdeactivate virtualenvwrapper.user_scripts creating /root/.virtualenvs/preactivate virtualenvwrapper.user_scripts creating /root/.virtualenvs/postactivate virtualenvwrapper.user_scripts creating /root/.virtualenvs/get_env_details ➜ ~ (3) 测试是否成功12345678910111213141516➜ ~ mkvirtualenv test01 -p python3 # -p 参数指定 python 版本 Using base prefix &apos;/usr&apos; No LICENSE.txt / LICENSE found in source New python executable in /root/.virtualenvs/test01/bin/python3.6 Also creating executable in /root/.virtualenvs/test01/bin/python Installing setuptools, pip, wheel... done. virtualenvwrapper.user_scripts creating /root/.virtualenvs/test01/bin/predeactivate virtualenvwrapper.user_scripts creating /root/.virtualenvs/test01/bin/postdeactivate virtualenvwrapper.user_scripts creating /root/.virtualenvs/test01/bin/preactivate virtualenvwrapper.user_scripts creating /root/.virtualenvs/test01/bin/postactivate virtualenvwrapper.user_scripts creating /root/.virtualenvs/test01/bin/get_env_details (test01) ➜ ~ (test01) ➜ ~ deactivate ➜ ~ workon test01 (test01) ➜ ~ (4)确定下虚拟环境内，python的路径，为在supervisor配置文件里使用12345(test01) ➜ ~ where is python is not found /root/.virtualenvs/test01/bin/python /usr/bin/python (test01) ➜ ~]]></content>
      <categories>
        <category>「原创分享」</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[生产环境CentOs7安装docker记实]]></title>
    <url>%2F%2Fp%2Fad3d805f.html</url>
    <content type="text"><![CDATA[生产环境CentOs7安装docker记实记录下，下次直接C、V使用 前置检查12345[root@web-95-61 /]# cat /etc/redhat-release CentOS Linux release 7.6.1810 (Core) [root@web-95-61 /]# uname -a Linux web-95-61 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux [root@web-95-61 /]# 配置国内yum源参考：https://blog.csdn.net/inslow/article/details/54177191https://yeasy.gitbooks.io/docker_practice/install/centos.html 12345678[admin@web-95-61 ~]$ su Password: [root@web-95-61 /]# cd /etc/yum.repos.d/ [root@web-95-61 yum.repos.d]# ls baofoo-centos-7.repo [root@web-95-61 yum.repos.d]# wget http://mirrors.163.com/.help/CentOS7-Base-163.repo[root@web-95-61 yum.repos.d]# yum makecache[root@web-95-61 yum.repos.d]# yum -y update 安装docker参考：https://qizhanming.com/blog/2019/01/25/how-to-install-docker-ce-on-centos-7https://yeasy.gitbooks.io/docker_practice/install/centos.html 卸载旧版本旧版本的 Docker 被叫做 docker 或 docker-engine，如果您安装了旧版本的 Docker ，您需要卸载掉它。12345678$ sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engine 旧版本的内容在 /var/lib/docker 下，目录中的镜像(images), 容器(containers), 存储卷(volumes), 和 网络配置（networks）都可以保留。 Docker CE 包，目前的包名为 docker-ce 安装准备为了方便添加软件源，支持 devicemapper 存储类型，安装如下软件包1234$ sudo yum update$ sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2 添加 yum 软件源123456789$ sudo yum-config-manager \ --add-repo \ https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo# 官方源# $ sudo yum-config-manager \# --add-repo \# https://download.docker.com/linux/centos/docker-ce.repo (可选)如果需要测试版本的 Docker CE 请使用以下命令：1$ sudo yum-config-manager --enable docker-ce-test 如果需要每日构建版本的 Docker CE 请使用以下命令：1$ sudo yum-config-manager --enable docker-ce-nightly 安装 Docker CE更新一下 yum 软件源的缓存，并安装 Docker。123$ sudo yum makecache fast$ sudo yum update$ sudo yum install docker-ce 至此，Docker 已经安装完成了，Docker 服务是没有启动的，操作系统里的 docker 组被创建，但是没有用户在这个组里。注意:默认的 docker 组是没有用户的（也就是说需要使用 sudo 才能使用 docker 命令）。您可以将用户添加到 docker 组中（此用户就可以直接使用 docker 命令了）。（可选）加入 docker 用户组命令1$ sudo usermod -aG docker USER_NAME 用户更新组信息后，重新登录系统即可生效。 启动 Docker如果想添加到开机启动1$ sudo systemctl enable docker 启动 docker 服务1$ sudo systemctl start docker 验证安装1$ sudo docker run hello-world 使用rancker管理，成为主机点击添加主机: 复制 新机器： 总图: 优化：Linux永久关闭swap 什么是swap:https://www.cnblogs.com/kerrycode/p/5246383.htmlLinux内核为了提高读写效率与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。即使你的程序运行结束后，Cache Memory也不会自动释放。这就会导致你在Linux系统中程序频繁读写文件后，你会发现可用物理内存变少。当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到Swap空间中，等到那些程序要运行时，再从Swap分区中恢复保存的数据到内存中。这样，系统总是在物理内存不够时，才进行Swap交换。 使用了docker 最好关闭swap，内存会超分，而且性能会很差。]]></content>
      <categories>
        <category>「原创分享」</category>
      </categories>
  </entry>
</search>
